#####################################################
#           THIS FILE IS AUTOGENERATED              #
# Update the template or the script, not this file! #
#####################################################
# Unique Identifier for the Head Node + Workers
cluster_name: marin-us-east5-a

# Maximum Workers (excluding Head Node)
max_workers: 1024


auth:
  ssh_private_key:  ~/.ssh/marin_ray_cluster.pem
  ssh_public_key:  ~/.ssh/marin_ray_cluster.pub
  ssh_user: ray


# Configure GCP
provider:
  type: gcp
  region: us-east5
  availability_zone: us-east5-a
  project_id: hai-gcp-models


docker:
    image: "us-east5-docker.pkg.dev/hai-gcp-models/marin/marin_cluster:20250721"
    container_name: "ray_docker"
    pull_before_run: true
    worker_run_options:
      - --privileged
      - --ulimit memlock=-1:-1  #
      - --ulimit nofile=1048576:1048576
      - --shm-size=32gb
      - -v
      - "/tmp:/tmp"
      # this lets the worker run docker commands and have them run as sibling containers
      - -v "/var/run/docker.sock:/var/run/docker.sock"
    head_run_options:
      - --privileged
      - -v "/tmp:/tmp"
      - --ulimit nofile=1048576:1048576
      - -e RAY_TPU_MAX_CONCURRENT_ACTIVE_CONNECTIONS=64


initialization_commands:
  - yes | gcloud auth configure-docker us-east5-docker.pkg.dev
  - which docker || (curl -fsSL https://get.docker.com -o get-docker.sh; sudo sh get-docker.sh; sudo usermod -aG docker $USER; sudo systemctl restart docker -f)
  # always run this because ray doesn't run with sudo
  - sudo usermod -aG docker $USER
  # we want to launch docker containers from inside docker, which means we need to loosen the permissions on the docker
  # socket. This isn't the best security practice, but it's the easiest way to get this working.
  - sudo chmod 666 /var/run/docker.sock

setup_commands:
  # set the GCP project because it's not injected by default
  - gcloud config set project hai-gcp-models
  - gcloud config set compute/region us-east5
  - gcloud config set compute/zone us-east5-a
  - mkdir $HOME/.cache/huggingface -p
  - gcloud secrets versions access latest --secret=HF_TOKEN > $HOME/.cache/huggingface/token
  - mkdir $HOME/.cache/openai -p
  - gcloud secrets versions access latest --secret=OPENAI_API_KEY > $HOME/.cache/openai/token
  - echo 'export MARIN_PREFIX="gs://marin-us-east5"' >> $HOME/.bashrc
  - echo 'export BUCKET="marin-us-east5"' >> $HOME/.bashrc
  # cf https://github.com/ray-project/ray/blob/0bc6ec86ffd0fc0d4e43fb339ffe0ac03ee5531b/python/ray/autoscaler/_private/constants.py#L66
  # this is set to 30s by default, which is much too short for our use case
  - echo 'export AUTOSCALER_HEARTBEAT_TIMEOUT_S=600' >> $HOME/.bashrc
  - echo 'export TPU_MIN_LOG_LEVEL=3' >> $HOME/.bashrc
  - echo 'export TPU_STDERR_LOG_LEVEL=3' >> $HOME/.bashrc
  - gcsfuse --implicit-dirs --only-dir gcsfuse_mount $BUCKET /opt/gcsfuse_mount || true
  - mkdir -p /home/ray/.ssh && gcloud compute project-info describe --format="value(commonInstanceMetadata.items[?key==\"ssh-keys\"].value)" > /home/ray/.ssh/authorized_keys && chmod 600 /home/ray/.ssh/authorized_keys
  - gcloud secrets versions access latest --secret=RAY_CLUSTER_PUBLIC_KEY > ~/.ssh/marin_ray_cluster.pub

# Set Head Node == `ray_head_default`
head_node_type: head_default


# List of Available Node Types
available_node_types:
  # Head Node =>> On-Demand, sets Min/Max Workers = 0 (Prevent Scheduling Tasks on Head Node)
  head_default:
    min_workers: 0
    max_workers: 0
    resources: {"CPU": 0, "head_node": 1}

    # GCP-Specific Configuration; by default, Ray will configure unspecified fields (e.g., subnets, ssh-keys)
    #   => Ref: https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
    node_config:
      machineType: n2-standard-8

      # Create a Persistent Disk w/ 200 GBs
      disks:
        - boot: true
          autoDelete: true
          type: PERSISTENT
          initializeParams:
            diskSizeGb: 200

            # Set Source Image =>> Ubuntu 22.04 Base VM
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts
  tpu_worker:
    max_workers: 1024
    min_workers: 4
    node_config:
      acceleratorType: v5p-8
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_8:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-8
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_16:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-16
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_32:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-32
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_64:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-64
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_128:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-128
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_256:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-256
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_512:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-512
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_1024:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5p-1024
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5p_2048:
    max_workers: 1024
    min_workers: 2
    node_config:
      acceleratorType: v5p-2048
      runtimeVersion: v2-alpha-tpuv5
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4
