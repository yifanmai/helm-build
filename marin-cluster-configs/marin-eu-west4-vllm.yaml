#####################################################
#           THIS FILE IS AUTOGENERATED              #
# Update the template or the script, not this file! #
#####################################################
# Unique Identifier for the Head Node + Workers
cluster_name: marin-eu-west4-vllm

# Maximum Workers (excluding Head Node)
max_workers: 1024

auth:
  ssh_private_key:  ~/.ssh/marin_ray_cluster.pem
  ssh_public_key:  ~/.ssh/marin_ray_cluster.pub
  ssh_user: ray

# Configure GCP
provider:
  type: gcp
  region: europe-west4
  availability_zone: europe-west4-b
  project_id: hai-gcp-models

docker:
    image: "europe-west4-docker.pkg.dev/hai-gcp-models/marin/marin_vllm:7fab502e"
    container_name: "ray_docker"
    pull_before_run: true
    worker_run_options:
        - --privileged
        - --ulimit memlock=-1:-1  #
        - --shm-size=200gb
        - -v
        - "/tmp:/tmp"
        # this lets the worker run docker commands and have them run as sibling containers
        - -v "/var/run/docker.sock:/var/run/docker.sock"
    head_run_options:
      - --privileged
      - -v "/tmp:/tmp"


initialization_commands:
  - yes | gcloud auth configure-docker europe-west4-docker.pkg.dev
  - which docker || (curl -fsSL https://get.docker.com -o get-docker.sh; sudo sh get-docker.sh; sudo usermod -aG docker $USER; sudo systemctl restart docker -f)
  # always run this because ray doesn't run with sudo
  - sudo usermod -aG docker $USER
  # we want to launch docker containers from inside docker, which means we need to loosen the permissions on the docker
  # socket. This isn't the best security practice, but it's the easiest way to get this working.
  - sudo chmod 666 /var/run/docker.sock
  # Install Google Cloud Ops Agent
  - curl -sS https://dl.google.com/cloudagents/add-google-cloud-ops-agent-repo.sh -o add-google-cloud-ops-agent-repo.sh
  - sudo bash add-google-cloud-ops-agent-repo.sh --also-install
  - gcloud secrets versions access latest --secret=RAY_CLUSTER_GOOGLE_CLOUD_OPS_AGENT_CONFIG --out-file=google-cloud-ops-agent-config.yaml
  - sudo mv google-cloud-ops-agent-config.yaml /etc/google-cloud-ops-agent/config.yaml
  - sudo systemctl restart google-cloud-ops-agent

setup_commands:
  # set the GCP project because it's not injected by default
  # NOTE(Chris): Have to include /root/gcloud/google-cloud-sdk/bin prefix because
  # there is some bug with the PATH not propagating from the Dockerfile to Ray.
  - /root/gcloud/google-cloud-sdk/bin/gcloud config set project hai-gcp-models
  - /root/gcloud/google-cloud-sdk/bin/gcloud config set compute/region europe-west4
  - /root/gcloud/google-cloud-sdk/bin/gcloud config set compute/zone europe-west4-b
  - mkdir $HOME/.cache/huggingface -p
  - /root/gcloud/google-cloud-sdk/bin/gcloud secrets versions access latest --secret=HF_TOKEN > $HOME/.cache/huggingface/token
  - mkdir $HOME/.cache/openai -p
  - /root/gcloud/google-cloud-sdk/bin/gcloud secrets versions access latest --secret=OPENAI_API_KEY > $HOME/.cache/openai/token
  - echo 'export MARIN_PREFIX="gs://marin-eu-west4"' >> $HOME/.bashrc
  - echo 'export BUCKET="marin-eu-west4"' >> $HOME/.bashrc
  - gcsfuse --implicit-dirs --cache-dir /dev/shm --file-cache-max-size-mb 160000 --client-protocol grpc --only-dir gcsfuse_mount $BUCKET /opt/gcsfuse_mount || true
  - mkdir -p ~/.ssh && /root/gcloud/google-cloud-sdk/bin/gcloud compute project-info describe --format="value(commonInstanceMetadata.items[?key==\"ssh-keys\"].value)" > ~/.ssh/authorized_keys && chmod 600 ~/.ssh/authorized_keys
  # Using /home/ray/.ssh/ since auth.ssh_user is set to 'ray' and we need to ensure the key is in the correct user's home directory else we get an error
  - /root/gcloud/google-cloud-sdk/bin/gcloud secrets versions access latest --secret=RAY_CLUSTER_PUBLIC_KEY > ~/.ssh/marin_ray_cluster.pub

# Set Head Node == `ray_head_default`
head_node_type: head_default


# List of Available Node Types
available_node_types:
  # Head Node =>> On-Demand, sets Min/Max Workers = 0 (Prevent Scheduling Tasks on Head Node)
  head_default:
    min_workers: 0
    max_workers: 0
    resources: {"CPU": 1}

    # GCP-Specific Configuration; by default, Ray will configure unspecified fields (e.g., subnets, ssh-keys)
    #   => Ref: https://cloud.google.com/compute/docs/reference/rest/v1/instances/insert
    node_config:
      machineType: n2-standard-8

      # Create a Persistent Disk w/ 200 GBs
      disks:
        - boot: true
          autoDelete: true
          type: PERSISTENT
          initializeParams:
            diskSizeGb: 200

            # Set Source Image =>> Ubuntu 22.04 Base VM
            sourceImage: projects/ubuntu-os-cloud/global/images/family/ubuntu-2204-lts
  tpu_worker:
    max_workers: 1024
    min_workers: 2
    node_config:
      acceleratorType: v5litepod-4
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_8:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-8
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_16:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-16
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_32:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-32
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_64:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-64
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_128:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-128
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4

  tpu_slice_v5e_256:
    max_workers: 1024
    min_workers: 0
    node_config:
      acceleratorType: v5litepod-256
      runtimeVersion: v2-alpha-tpuv5-lite
      schedulingConfig:
        preemptible: true
    resources:
      CPU: 120
      TPU: 4
